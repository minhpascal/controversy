\documentclass{article}[12pt]
\usepackage{palatino}
\begin{document}
\noindent\textbf{Graham Dyer}, CS 397, FA15\hfill{\tt{gdyer2}\tt{@}\tt{illinois}\tt{.}\tt{edu}}
\hrule
\vspace{5mm}
This semester, Ismini and I continued our work on controversy-detection within news articles. Much progress was made in all areas, but most notably were efforts in building a dataset, improving the scoring algorithm, and implementing more precise retrieval of social content. In the beginning of the semester, the demo \footnote{https://controversy.2pitau.org} was also improved.

Our work in building a dataset for training was the most apparent progress. The unannotated dataset consists of \textbf{X} unique news articles including their metadata and comments, as well as relevant tweets. Retrieval of some tweets and all news articles is keyword-based. Thus, a single term is used to query and return a list of up to 10 news articles and 500 tweets. These tweets and the URL of each article as well as attributes such as date, author, and the article's abstract are retained. We then scrape the article's webpage to obtained the full-text of the article. Finally, a second twitter query is performed using the article's title without stopwords. All of these data are saved within a schema to a persistent database. Non-tweet content is obtained from The New York Times, Reuters, and the Associated Press. Article webpage comments contain many metadata variables, all of which we save. In all twitter queries, we also save metadata related to the tweet itself (retweets, favorites, etc.) and that related to the author (location, followers, etc.). All of this content is stored in a persistent database and is optimized for training.

The largest obstacle in curating the unannotated dataset was speed. Given the considerable size of the data (and single-core nature of the server!), building an efficient scraper was key. However, minimizing the time spent on this aspect of the project (so we could start annotation and thus improve the scoring function) was equally important. Speed in the former case was improved by not running various analyses on the retrieved content. We do as little post-processing as possible: no scoring occurs, no linguistic or sentiment features are noted, and organizing into a schema-based database is avoided. Another consideration was not using a ranking function such as Okapi BM25 to filter tweets by their relevance to the articles. These procedures can be done later and, in the case of sentiment, more efficiently in a batch.

\textit{not done yet\ldots}


\end{document}
